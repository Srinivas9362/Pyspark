{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b45139b-a172-42a1-a278-74885196fbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Heelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8d5c44-bc66-45ca-ab49-e7e21c36fc3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PYSPARK INTERVIEW QUESTIONS from Ansh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533c2e17-9054-4732-ad33-bafeca9265c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f2c347-0e58-4bcd-99cf-5a6a24029749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a207944a-3b80-4996-8aa5-91245f91eec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"101\", \"2023-12-01\", 100), (\"101\", \"2023-12-02\", 150), \n",
    "        (\"102\", \"2023-12-01\", 200), (\"102\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2bfd50-6065-4a9d-bef2-3ca88a1dff83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('date', col('date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df29a93-9552-4845-a9e7-898c0c59f0ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('date',to_date('date')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1246dd6d-9afb-453d-8476-903005f5db7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5072102-6dcd-45bf-9a6c-fa84f87523a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dropDuplicates(subset=['product_id']).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f4718f-2a1e-49fb-8959-45460a89f7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2987d0c6-6c2a-41bd-9b7f-81ad947d1baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.orderBy(['product_id','date'], ascending=[1,0]).dropDuplicates(['product_id'])\n",
    "\n",
    "# df = df.orderBy(['product_id', 'date'], ascending=[True, False])\\\n",
    "    #    .dropDuplicates(['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a8cbdf-ebe5-4136-824f-84641e52ce86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0705055a-d6de-4c0c-9335-10f7406a03d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. While processing data from multiple files with inconsistent schemas, you need to merge them into a single DataFrame. How would you handle this inconsistency in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30105a1b-e31c-499c-b89c-914a7abf62c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "    .option('mergeSchema',True)\\\n",
    "    .load('/Volumes/datasets/practice/internal_files/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acfa294-4d56-4544-ab35-0ef026d9dd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a32b255c-e97b-482b-986c-62b623856bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#MapReduce\n",
    "Writes intermediate results to disk between each stage (e.g., between Mapper and Reducer).\n",
    "\n",
    "The Reducer reads data from disk, which makes the process time-consuming.\n",
    "\n",
    "It is primarily designed for batch processing.\n",
    "\n",
    "Due to frequent disk I/O, MapReduce is slower than in-memory frameworks like Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20491dc0-dab8-4604-8c46-8b912eb9c339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Apache Spark\n",
    "Performs most computations in memory, writing to disk only during final write operations or when memory overflows.\n",
    "\n",
    "Faster compared to MapReduce due to reduced disk I/O.\n",
    "\n",
    "Less time-consuming for iterative and complex computations.\n",
    "\n",
    "Supports both batch and streaming processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b500165-8157-4c0a-a1fd-8c8498c0cc50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#4. You are working with a real-time data pipeline, and you notice missing values in your streaming data Column - Category. How would you handle null or missing values in such a scenario?\n",
    "\n",
    "df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ce3b2c-424f-43a0-9876-323b981e490e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream = spark.read.format('csv')\\\n",
    "    .option('InferSchema',True)\\\n",
    "    .option('Header','True')\\\n",
    "    .load('/Volumes/datasets/practice/internal_files/BigMart Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb418cd5-e709-4f24-981c-0ccd4e2cc0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_stream.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cc206a-5854-4339-838d-51702a3915a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_st = df_stream.fillna({'Item_Weight':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dac8913-7fe6-4669-ae57-d61a720b6762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_st.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa493556-4f1f-495f-a8a8-705a687acf8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7361e3-e5de-4bed-b5e3-126278ca0ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user4\", 10), (\"user2\", 3)]\n",
    "columns = [\"user_id\", \"actions\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722c1112-60ac-46f7-a42e-cd43d80fdb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('user_id').agg(sum('actions').alias('Total_Actions')).orderBy('Total_Actions',ascending=False).limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e62675-d923-4098-8951-2fe98bda9b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbfaa7c-c06b-437c-a268-c004b36b580a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2023-12-01\", 100), (\"cust2\", \"2023-12-02\", 150),\n",
    "        (\"cust1\", \"2023-12-03\", 200), (\"cust2\", \"2023-12-04\", 250)]\n",
    "columns = [\"customer_id\", \"transaction_date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fc68f6-bca2-4ee9-a711-68cff4a9d54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('customer_id').agg(max('transaction_date').alias('Last_Transaction')).display()\n",
    "# df.groupBy('customer_id').agg(sum('sales').alias('Total_Sales')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7ea22e-ed0d-40d1-87fa-2002b3671640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd36f82e-4a76-47b8-ac4d-9a4cfdce61ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('dense_rank', dense_rank().over(Window.partitionBy('customer_id').orderBy(col('transaction_date').desc()))).filter(col('dense_rank')==1).drop('dense_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913b276c-37c2-4294-9640-e9643db8c8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ba3a8c-db0f-47f1-a9b0-ce9d65941f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#7. You need to identify customers who haven’t made any purchases in the last 30 days. How would you filter such customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc537b9-34c3-414b-9e1f-af3a244c687e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"cust1\", \"2025-12-01\"), (\"cust2\", \"2024-11-20\"), (\"cust3\", \"2024-11-25\")]\n",
    "columns = [\"customer_id\", \"last_purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c74a6fe-de50-48ab-8c64-d15f4f009ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 =df.withColumn('last_purchase_date', col('last_purchase_date').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728a5599-12cc-4f5e-a4e8-b0e5aca653e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f54fed-5502-40b4-a28d-04474493fdc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.withColumn('gap',date_diff(current_date(), 'last_purchase_date')).filter(col('gap')>30).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364fdaff-dd19-4945-abe6-b62e9da31905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, to_date, col, date_sub\n",
    "\n",
    "# Convert string date to DateType if not already\n",
    "df = df.withColumn(\"last_purchase_date\", to_date(col(\"last_purchase_date\")))\n",
    "\n",
    "# Filter customers whose last purchase date is older than 30 days ago\n",
    "inactive_customers = df.filter(col(\"last_purchase_date\") < date_sub(current_date(), 30))\n",
    "\n",
    "inactive_customers.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09aa9e25-ea9c-4216-b3c7-00ca3f9b8e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(col('last_purchase_date') > date_sub(current_date(),90)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c72134e-79e9-474f-84a5-53e25602edf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#8. While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1355b83-6211-43c7-9c2e-df6e3058891f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfdede6-c370-47b4-b2cd-483943ddb680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.withColumn('lists', split(col('feedback'), ' '))\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f84e38-0e62-4179-b6a4-32f7c3d1f38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('Explode_col',explode('lists'))\n",
    "df1.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "078db9d6-95ae-4fd0-99af-90652a8103b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.withColumn('Explode_col',trim(col('Explode_col'))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884c1c43-c7bc-4055-8939-fc25b3a80dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('Explode_col',regexp_replace(col('Explode_col'),',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15533ed8-71d1-4c60-8d7f-83811a68eb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.groupBy('Explode_col').agg(count('Explode_col').alias('Count')).filter(col('Count')>1).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df81296b-5214-4058-bf41-5e6c4b8aa39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8017a6-5824-4225-8cd1-fe2ac7e7b1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fca7aaa-af83-4e8c-9036-d039559b204a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Cum_sum', sum('sales').over(Window.partitionBy('product_id').orderBy('date').rowsBetween(Window.unboundedPreceding, Window.currentRow))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8fd80d-d27e-43c0-a0b1-7a3083594c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Cum_sum', sum('sales').over(Window.partitionBy('product_id').orderBy('date'))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7d895d0-eaee-4c60-9d6f-ef81ad19ef59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting the original order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6354ddf2-c12c-4839-a5cc-57d1b389f819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7e7f30-c551-441b-860c-e4ccd29eb40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('drop_dup', row_number().over(Window.partitionBy('name','age').orderBy('name'))).filter(col('drop_dup')==1).drop('drop_dup').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f0420b7-bd7f-40ee-b1e2-47d1f01e9fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('dedup',row_number().over(Window.partitionBy('name').orderBy('age'))).filter(col('dedup')==1).drop('dedup').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ea2f53a-57d0-432d-8587-187401d39418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201ea08c-74b1-459a-8fdc-5448070895f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f50814-5923-420c-b0d7-44189434069a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('user_id').agg(avg('duration').alias('avg_duration')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05ccef40-eff4-4cf8-97da-ff51a430af89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cd3650-07ef-4098-8381-e47f7a4c4b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139faf81-0462-47e7-8b62-cbe506e5096f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = df.withColumn('date_col', to_date('date'))\\\n",
    "    .withColumn('month',month('date_col'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e447457d-d92a-447d-9256-e1742471af13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0ad546-3011-48cb-8a03-fd9ad891e3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.groupBy('month','product_id').agg(sum('sales').alias('highest_sales')).orderBy(col('highest_sales').desc()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84d77f7f-93ee-49e2-8e49-434fc1071ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and avoid data corruption in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2188cbaa-31b8-4225-a989-04f0ded8c59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').load('/Volumes/datasets/practice/internal_files/output/data.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989f2d87-2def-44be-a6aa-2ed98d08cc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95cbf6eb-8989-42f4-a49b-d36eb2db288b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, '/Volumes/datasets/practice/internal_files/output/data.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0dc85d-d8e5-46d5-a9ff-ac8b914e51ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable.alias('trg').merge(df.alias('src'), 'trg.id = src.id')\\\n",
    "          .whenNotMatchedInsertAll()\\\n",
    "          .whenMatchedUpdateAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5770ae-f766-4d5e-a1a4-e3005bbfe9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e1181f-5c77-44cc-8636-b7a28581f695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet')\\\n",
    "    .option('inferSchem',True)\\\n",
    "    .load('/Volumes/datasets/practice/internal_files/output/data.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f6e3de-5ec1-49fd-97e8-38e799b15633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaed2c6c-b504-47b0-a60a-396e503597af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7318a7e8-2db7-4af4-8456-4ad2d71bcbe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .option('mode','DROPMALFORMED')\\\n",
    "    .load('/Volumes/datasets/practice/internal_files/order_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f2bdd3-54ea-40fa-a122-1c77ff45e1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec03dbc-bfc9-43ce-9e09-7391fb55791e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Difference between RDD and DATAFRAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e4b9bf-859e-4cdf-9432-f7b1e2841ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature                  | RDD (Resilient Distributed Dataset)         | DataFrame                                               |\n",
    "| ------------------------ | ------------------------------------------- | ------------------------------------------------------- |\n",
    "| **Abstraction Level**    | Low-level (object-oriented)                 | High-level (tabular like SQL)                           |\n",
    "| **Ease of Use**          | More complex (requires more code)           | Easy (SQL-like, less code)                              |\n",
    "| **Data Format**          | Distributed collection of objects           | Distributed table with named columns                    |\n",
    "| **Optimization**         | No automatic optimization                   | Uses Catalyst optimizer & Tungsten engine               |\n",
    "| **Performance**          | Slower (manual tuning needed)               | Faster (optimized execution plan)                       |\n",
    "| **Schema Support**       | No schema                                   | Schema-aware                                            |\n",
    "| **Type Safety**          | Type-safe (in Scala/Java)                   | Partially type-safe (better in Scala than Python)       |\n",
    "| **Use Case**             | Complex data operations, custom processing  | Structured data, ETL, SQL operations                    |\n",
    "| **Supported Operations** | Low-level transformations (`map`, `filter`) | High-level ops (`select`, `groupBy`, `join`, etc.)      |\n",
    "| **API Style**            | Functional programming                      | Declarative/SQL-style                                   |\n",
    "| **Best For**             | Fine-grained data transformations           | Standard ETL, analytics, performance-critical workflows |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d53644-e80c-4af1-acdc-30d5a015d7c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature                 | DataFrame                                      | Dataset                                               |\n",
    "| ----------------------- | ---------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Abstraction Level**   | High-level (tabular data, like SQL table)      | Type-safe, object-oriented + tabular                  |\n",
    "| **Language Support**    | Supported in **Scala, Java, Python, R**        | **Scala and Java only**                               |\n",
    "| **Type Safety**         | **No** compile-time type checking              | **Yes**, with compile-time type safety                |\n",
    "| **Compile-Time Errors** | Detected only at runtime                       | Detected at compile time (for Scala/Java)             |\n",
    "| **Ease of Use**         | Easier to use (less boilerplate)               | Requires defining case classes or custom types        |\n",
    "| **Performance**         | Similar performance (uses Catalyst + Tungsten) | Similar performance (optimized query plan)            |\n",
    "| **Serialization**       | Uses Tungsten's off-heap binary format         | Uses Encoders (slightly more overhead)                |\n",
    "| **Data Handling**       | Best for **structured data, ETL**              | Best for **type-safe transformations, complex logic** |\n",
    "| **Interoperability**    | Easily converted to/from RDD and Dataset       | Can be converted to/from DataFrame and RDD            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91af9f8c-dfe4-4999-84b4-e3b0fdd825ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ What is Query Optimization in Spark?\n",
    "Query Optimization in Spark is the process of transforming user-written logical plans into efficient physical plans that run faster with fewer resources.\n",
    "\n",
    "This optimization is done automatically by the Catalyst Optimizer when you write Spark SQL or DataFrame queries.\n",
    "\n",
    "| Stage                       | Description                                                                       |\n",
    "| --------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **1. Analysis**             | Converts SQL/DataFrame code into an **unresolved logical plan** using schema info |\n",
    "| **2. Logical Optimization** | Applies **rules** like constant folding, predicate pushdown, etc.                 |\n",
    "| **3. Physical Planning**    | Generates one or more **physical plans** (e.g., hash join vs sort-merge join)     |\n",
    "| **4. Code Generation**      | Uses **Tungsten engine** to generate optimized bytecode (JVM) for execution       |\n",
    "\n",
    "\n",
    "🔧 Optimization Techniques (Automatically Done)\n",
    "| Optimization Name                  | What It Does                                            |\n",
    "| ---------------------------------- | ------------------------------------------------------- |\n",
    "| **Predicate Pushdown**             | Moves filters closer to the data source                 |\n",
    "| **Constant Folding**               | Evaluates constant expressions at compile time          |\n",
    "| **Projection Pruning**             | Selects only needed columns (avoids full data scans)    |\n",
    "| **Join Reordering**                | Picks the best join order to reduce shuffling           |\n",
    "| **Filter Pushdown in Parquet/ORC** | Applies filters while reading from disk to minimize I/O |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa16b870-e93a-43b3-b124-f4ff8e744b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f163a8-0971-46e8-aebc-ce8584ecf742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔥 Apache SparkSession – Explained Clearly\n",
    "A SparkSession is the entry point to programming with Spark using the DataFrame and Dataset API in Spark 2.x and above.\n",
    "\n",
    "Before Spark 2.0, you had to use:\n",
    "\n",
    "SparkContext (for core Spark features)\n",
    "\n",
    "SQLContext (for SQL features)\n",
    "\n",
    "But now, SparkSession replaces them all.\n",
    "\n",
    "✅ What is SparkSession?\n",
    "It's the gateway to all Spark functionalities.\n",
    "\n",
    "Used to:\n",
    "\n",
    "Read data from sources (CSV, JSON, Parquet, Hive, etc.)\n",
    "\n",
    "Create DataFrames\n",
    "\n",
    "Run SQL queries\n",
    "\n",
    "Configure Spark settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa6c94f-4b73-4b39-b76a-04ebb176b210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🧠 What are Transformations in Spark?\n",
    "In Spark, transformations are operations that return a new RDD or DataFrame from an existing one.\n",
    "They are of two types:\n",
    "\n",
    "Type\tMeaning\n",
    "Narrow\tData required to compute the result comes from a single partition\n",
    "Wide\tData required comes from multiple partitions → causes shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53308639-77a9-407a-a0dc-c80616edf970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature                       | **Narrow Transformation**            | **Wide Transformation**                            |\n",
    "| ----------------------------- | ------------------------------------ | -------------------------------------------------- |\n",
    "| **Data Movement**             | No data movement across partitions   | Requires data shuffle across partitions            |\n",
    "| **Dependency Type**           | One-to-one or few-to-one             | Many-to-one or many-to-many                        |\n",
    "| **Performance**               | Faster, more efficient               | Slower due to shuffle overhead                     |\n",
    "| **Examples**                  | `map`, `filter`, `union`, `coalesce` | `groupByKey`, `reduceByKey`, `join`, `repartition` |\n",
    "| **Shuffle**                   | ❌ No shuffle                         | ✅ Causes shuffle                                   |\n",
    "| **Task Scheduling**           | Simple linear execution              | Requires barrier/shuffle stages                    |\n",
    "| **Fault Tolerance (Lineage)** | Easier to recompute                  | Requires recomputation from multiple sources       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9d4b9c-1c5b-4c51-936c-d9ddb9ebc212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###| Feature                       | **Narrow Transformation**            | **Wide Transformation**                            |\n",
    "| ----------------------------- | ------------------------------------ | -------------------------------------------------- |\n",
    "| **Data Movement**             | No data movement across partitions   | Requires data shuffle across partitions            |\n",
    "| **Dependency Type**           | One-to-one or few-to-one             | Many-to-one or many-to-many                        |\n",
    "| **Performance**               | Faster, more efficient               | Slower due to shuffle overhead                     |\n",
    "| **Examples**                  | `map`, `filter`, `union`, `coalesce` | `groupByKey`, `reduceByKey`, `join`, `repartition` |\n",
    "| **Shuffle**                   | ❌ No shuffle                         | ✅ Causes shuffle                                   |\n",
    "| **Task Scheduling**           | Simple linear execution              | Requires barrier/shuffle stages                    |\n",
    "| **Fault Tolerance (Lineage)** | Easier to recompute                  | Requires recomputation from multiple sources       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b784e5-1768-418d-875f-e8ad25a88acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🧠 What Are They?\n",
    "Both coalesce() and repartition() are used to change the number of partitions in a DataFrame or RDD.\n",
    "\n",
    "| Function         | Purpose                                           |\n",
    "| ---------------- | ------------------------------------------------- |\n",
    "| `coalesce(n)`    | **Decrease** the number of partitions             |\n",
    "| `repartition(n)` | **Increase or decrease** the number of partitions |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae31e9ce-f095-4663-b6c5-62bd844383b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ Difference Between coalesce() vs repartition()\n",
    "| Feature             | `coalesce()`                              | `repartition()`                                     |\n",
    "| ------------------- | ----------------------------------------- | --------------------------------------------------- |\n",
    "| Purpose             | Reduce partitions (narrow transformation) | Increase or re-distribute (wide transformation)     |\n",
    "| Shuffle             | ❌ No full shuffle                         | ✅ Causes full shuffle                               |\n",
    "| Use Case            | Optimize performance when writing to disk | Ensure better parallelism before joins/aggregations |\n",
    "| Performance         | Faster (less data movement)               | Slower (due to shuffling)                           |\n",
    "| Best When           | Reducing partitions (e.g., from 200 → 10) | Rebalancing or increasing partitions                |\n",
    "| Transformation Type | Narrow                                    | Wide                                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9516a141-975b-4183-90b0-fb133a3a09c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#⚠️ Best Practices\n",
    "Use **coalesce()** when:\n",
    "\n",
    "You are only decreasing partitions.\n",
    "\n",
    "You want less data movement.\n",
    "\n",
    "You are writing fewer output files.\n",
    "\n",
    "Use **repartition()** when:\n",
    "\n",
    "You need better data distribution.\n",
    "\n",
    "You're performing joins or groupBy on large data.\n",
    "\n",
    "You want more parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a283596a-fc50-4a26-8317-313834dc62cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#💾 Spark cache() vs persist() – Detailed Explanation with Table\n",
    "Both cache() and persist() in Apache Spark are used to store the intermediate results of DataFrames or RDDs in memory (and/or disk) for reuse — to avoid recomputation and improve performance in iterative or multi-step pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6366831-e723-47e6-8015-c4f6b559ebdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ What is persist()?\n",
    "persist() gives you more control over how and where the data is stored.\n",
    "\n",
    "You can choose various StorageLevels, like:\n",
    "\n",
    "MEMORY_ONLY\n",
    "\n",
    "MEMORY_AND_DISK\n",
    "\n",
    "DISK_ONLY\n",
    "\n",
    "MEMORY_ONLY_SER (serialized form)\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "824cbc41-60ea-4e21-811c-c744445be58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#📊 cache() vs persist() – Comparison Table\n",
    "| Feature                   | `cache()`                                   | `persist()`                                     |\n",
    "| ------------------------- | ------------------------------------------- | ----------------------------------------------- |\n",
    "| **Storage Level**         | `MEMORY_AND_DISK` (default)                 | Customizable (`MEMORY_ONLY`, `DISK_ONLY`, etc.) |\n",
    "| **Control**               | Limited                                     | Full control over storage strategy              |\n",
    "| **Ease of Use**           | Very simple (no arguments)                  | Requires specifying `StorageLevel`              |\n",
    "| **Use Case**              | General caching when defaults are fine      | Specific tuning (e.g., memory-constrained jobs) |\n",
    "| **Serialization Options** | Not available                               | Can use serialized formats (`MEMORY_ONLY_SER`)  |\n",
    "| **Reusability**           | Both support reuse across multiple actions  | Both support reuse across multiple actions      |\n",
    "| **Unpersisting**          | Can use `.unpersist()` to remove from cache | Same with `.unpersist()`                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ef76d3-d4d4-4377-bfc8-6ce4fee25e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 What \"Memory\" and \"Disk\" Mean in Databricks / Spark\n",
    "When you call .cache() or .persist() on a DataFrame/RDD in Databricks (Spark), you're asking Spark to store the data in memory (RAM) or on local disk — on the worker nodes.\n",
    "\n",
    "Term\tMeaning in Databricks (Spark) Context\n",
    "Memory\tThe RAM available on the worker nodes in your Databricks cluster\n",
    "Disk\tThe local disk space on the same worker nodes (not S3/DBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f74ede-f888-4916-a911-2d89ffb8a093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87945eb7-6113-42e3-acd7-fbe7bcf08967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ Why Partitioning Matters in PySpark\n",
    "| Benefit                          | How Partitioning Helps                                              |\n",
    "| -------------------------------- | ------------------------------------------------------------------- |\n",
    "| ⚡ Faster Execution               | Enables **parallel processing** across worker nodes                 |\n",
    "| 🔁 Reduced Shuffle               | If partitioned properly (e.g., on join keys), reduces data movement |\n",
    "| 💽 Better Memory Usage           | Each partition fits into executor memory; avoids out-of-memory      |\n",
    "| 📊 Scalable Joins & Aggregations | Partitioned data reduces skew and improves load balancing           |\n",
    "| 🔁 Reuse Cached Data             | Partitions stay in memory for reuse if cached/persisted             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a119427d-77cf-4543-a761-2a59215c074f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#22. You have a dataset containing the names of employees and their departments. You need to find the department with the most employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0b8b4a-f210-4e7f-b1ab-3cb9de6c0d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Alice\", \"HR\"), (\"Bob\", \"Finance\"), (\"Charlie\", \"HR\"), (\"David\", \"Engineering\"), (\"Eve\", \"Finance\")]\n",
    "columns = [\"employee_name\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59917c4d-8a9b-4a77-9d5a-83e150c8b966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25945bed-cf9d-4967-bc28-3aa190d46229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('department').agg(count('employee_name').alias('Total_Employees')).sort('Total_Employees', ascending=False).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d20261-a696-4cc5-bb23-58f8fff86d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#23. While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you achieve this using a when condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7736b80d-a1c2-491c-9b11-8528238af42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86dc96d-ab6f-4fcb-94f2-c951cb9a65d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Classify_Flag', \n",
    "              when(col('sales')>=300,'High')\n",
    "             .when((col('sales')>=100) & (col('sales')<300),'Medium')\n",
    "             .otherwise('Low')).display()\n",
    "\n",
    "# df.withColumn('Classify_Flag',\n",
    "#     when(col('sales') >= 300, 'High')\n",
    "#     .when((col('sales') >= 100) & (col('sales') < 300), 'Medium')\n",
    "#     .otherwise('Low')\n",
    "# ).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95a5207e-2fed-43c1-9ddf-b4689acdb7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#24. While analyzing a large dataset, you need to create a new column that holds a timestamp of when the record was processed. How would you implement this and what can be the best USE CASE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e803696-588c-4a05-a2fe-c6632fbcc5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804b4008-ca69-4ac4-b3ca-dc82c9456d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('processing_Time',current_timestamp()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380fef45-5eee-4ed2-8ec8-98dde4fff4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#25. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it. How would you achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da12a3e3-57cd-452e-a5ea-83af01838d76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27688af9-1926-4ced-8556-705c80d5e836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createTempView('My_View_Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce7f37a-5d1b-43c2-84b0-07d408afdcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from My_View_Table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce303e9d-2a66-4752-a6b1-3242d9ceeb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#26. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it (FROM DIFFERENT NOTEBOOKS AS WELL)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7809b71c-0580-49bd-ac0e-966489051eab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.createOrReplaceGlobalTempView('Table_global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd63ad43-9271-4ac4-a037-6e419dccfbfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#27. You need to query data from a PySpark DataFrame using SQL, but the data includes a nested structure. How would you flatten the data for easier querying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b967ad1-c077-4e4c-ab84-93acdc57e751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"product1\", {\"price\": 100, \"quantity\": 2}), \n",
    "        (\"product2\", {\"price\": 200, \"quantity\": 3})]\n",
    "columns = [\"product_id\", \"product_info\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c6108d-8a2c-4f4f-bb55-9edaa8c5dc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('product_id', \"product_info.price\", \"product_info.quantity\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71cb2004-ebef-4645-b676-7b6de13b830e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select('product_id', \"product_info.price\", \"product_info.quantity\").createTempView('flat_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3744bb3a-1178-4660-87c9-a0f8670c7c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from flat_view;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c72b45a-a803-48b3-b809-5b55dfc4c3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#28. You are ingesting data from an external API in JSON format where the schema is inconsistent. How would you handle this situation to ensure a robust pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78d5109-3d81-4c4f-97e0-bd26e62c46c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Define flexible schema (only core fields)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 2. Read raw JSON as string\n",
    "raw_df = spark.read.text(\"s3://path/to/json\")\n",
    "\n",
    "# 3. Parse JSON with schema\n",
    "parsed_df = raw_df.withColumn(\"json_data\", from_json(col(\"value\"), schema)).select(\"json_data.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4c6661-45ee-49cc-b641-df8825383e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#29. While reading data from Parquet, you need to optimize performance by partitioning the data based on a column. How would you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a959c6ca-3f91-4bcf-aa23-d241ac8e331e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= spark.read.format('csv').option('inferSchema',True).option('header', True).load('/Volumes/datasets/practice/internal_files/BigMart Sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64e4409-b1d9-4c89-a60f-a0471af425fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432e9507-fc55-486a-be73-90b2515bd877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode('append').partitionBy('Outlet_Size').save('/Volumes/datasets/practice/internal_files/output2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df3edff-2f4f-439e-ab25-bf509d7f8caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.format('parquet').load('/Volumes/datasets/practice/internal_files/output2/Outlet_Size=High/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44004ec-3773-4293-960c-41cfa5784a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05cb0188-0b17-40e9-b5b4-5349da7f38b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#30. You are working with a large dataset in Parquet format and need to ensure that the data is written in an optimized manner with proper compression. How would you accomplish this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854ece95-7e6c-4649-abbc-5516eaf69569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet').option('compression','snappy').mode('append').partitionBy('Outlet_Size').save('/Volumes/datasets/practice/internal_files/output4/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bc8739d-caa4-4b26-8a14-e3d96be331d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#31. Your company uses a large-scale data pipeline that reads from Delta tables and processes data using complex aggregations. However, performance is becoming an issue due to the growing dataset size. How would you optimize the performance of the pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa5ea37-261f-471b-9687-7f0af05b8f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ 1. Optimize Delta Table Storage\n",
    "Partitioning: Re-evaluate your partitioning strategy.\n",
    "\n",
    "Use columns with high cardinality but even distribution.\n",
    "\n",
    "Avoid over-partitioning which can lead to many small files (too many metadata operations).\n",
    "\n",
    "Z-Ordering: Apply OPTIMIZE ... ZORDER BY on frequently filtered or joined columns.\n",
    "\n",
    "Improves data skipping and reduces scan time.\n",
    "\n",
    "File Compaction: Periodically run OPTIMIZE to compact small files.\n",
    "\n",
    "Reduces shuffle and I/O overhead during reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ec9c2a8-4cbb-42bc-a633-cb59a433dde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ 2. Query Optimization Techniques\n",
    "Push Down Predicates: Ensure filters are pushed down early (e.g., df.filter() before joins/aggregations).\n",
    "\n",
    "Use Broadcast Join (if one side is small): Reduces shuffle and improves join performance.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df_large.join(broadcast(df_small), on=\"key\")\n",
    "Cache Intermediate Results: If intermediate DataFrames are reused, persist/cache them.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da4c2b9-3ae7-4672-934e-48a57f1a176b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ Why Are Broadcast Variables Used?\n",
    "In distributed computing, if you use a variable (e.g., a lookup dictionary) inside transformations like .map(), it gets shipped with every task, potentially causing network overhead and redundancy.\n",
    "\n",
    "Broadcast variables solve this by:\n",
    "\n",
    "Sending the variable only once to each executor.\n",
    "\n",
    "Storing it in-memory, making access fast and avoiding repeated transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0e4e38-8776-42ea-a669-90f50cb2dca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature       | Description                                                       |\n",
    "| ------------- | ----------------------------------------------------------------- |\n",
    "| Type          | Read-only shared variable                                         |\n",
    "| When to Use   | Small data used across many transformations (e.g., lookup tables) |\n",
    "| Benefit       | Reduces network traffic & improves job performance                |\n",
    "| Access Syntax | `broadcast_var.value`                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29be1f5a-e369-4821-8f8e-3863d201ebee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 df.show() vs df.collect() — Key Differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c433ede7-dd5c-4b71-85cf-693c366a5d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature            | `df.show()`                                       | `df.collect()`                                       |\n",
    "| ------------------ | ------------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Purpose**        | Displays a **preview** (default: 20 rows) of data | Collects **all rows** into driver memory             |\n",
    "| **Return Type**    | **None** (just prints output)                     | Returns a **list of Row objects**                    |\n",
    "| **Performance**    | Lightweight — good for **previewing data**        | Heavy — risky for large datasets                     |\n",
    "| **Memory Usage**   | Minimal (prints limited rows)                     | High — can crash driver if data is too big           |\n",
    "| **Common Use**     | Debugging, quick look at data                     | When you actually need to access full data in driver |\n",
    "| **Syntax Example** | `df.show(10)`                                     | `df.collect()`                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5de226d9-83e6-43cb-84dd-e8575df993c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 What is Lazy Evaluation in PySpark?\n",
    "Lazy Evaluation means that PySpark does not immediately execute your transformations (like select, filter, map, withColumn, etc.).\n",
    "Instead, it builds a logical plan (a DAG – Directed Acyclic Graph) and waits until an action (like show(), collect(), count(), etc.) is called to actually run the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536090b4-4cad-4d97-8528-d24ff0b7e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "df = spark.read.csv(\"sales.csv\", header=True)\n",
    "\n",
    "# These are transformations (lazy)\n",
    "df_filtered = df.filter(df[\"amount\"] > 1000)\n",
    "df_grouped = df_filtered.groupBy(\"region\").sum(\"amount\")\n",
    "\n",
    "# This is an action (triggers execution)\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eca05ff2-571e-44c5-acf1-46caee85be9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Benefit               | Explanation                                                                                |\n",
    "| --------------------- | ------------------------------------------------------------------------------------------ |\n",
    "| ✅ **Optimization**    | Spark can analyze the entire DAG and **optimize** the execution plan (Catalyst Optimizer). |\n",
    "| ✅ **Efficiency**      | Avoids unnecessary computations — unused transformations aren’t run.                       |\n",
    "| ✅ **Fault Tolerance** | Keeps lineage info so it can recompute lost partitions on failure.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94acaf11-d3f9-4bc8-963c-11429e29ef42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "df = spark.read.csv(\"sales.csv\", header=True)\n",
    "\n",
    "# These are transformations (lazy)\n",
    "df_filtered = df.filter(df[\"amount\"] > 1000)\n",
    "df_grouped = df_filtered.groupBy(\"region\").sum(\"amount\")\n",
    "\n",
    "# This is an action (triggers execution)\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a782dcbc-de3b-4ced-90a7-73a569a9cf35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Transformations(Lazy)\n",
    "These define a computation, but don’t trigger execution. They return a new DataFrame or RDD.\n",
    "| Transformation              | Description                                 |\n",
    "| --------------------------- | ------------------------------------------- |\n",
    "| `select()`                  | Select specific columns                     |\n",
    "| `filter()` / `where()`      | Filter rows based on a condition            |\n",
    "| `withColumn()`              | Add or modify a column                      |\n",
    "| `drop()`                    | Drop columns                                |\n",
    "| `groupBy()`                 | Group rows based on a column                |\n",
    "| `agg()`                     | Apply aggregations on grouped data          |\n",
    "| `orderBy()` / `sort()`      | Sort rows                                   |\n",
    "| `join()`                    | Join two DataFrames                         |\n",
    "| `distinct()`                | Remove duplicate rows                       |\n",
    "| `dropDuplicates()`          | Remove duplicates based on specific columns |\n",
    "| `limit()`                   | Limit number of rows                        |\n",
    "| `repartition()`             | Increase number of partitions               |\n",
    "| `coalesce()`                | Reduce number of partitions                 |\n",
    "| `union()` / `unionByName()` | Combine two DataFrames                      |\n",
    "| `explode()`                 | Flatten array or map columns                |\n",
    "| `alias()`                   | Rename columns (often used in `select()`)   |\n",
    "| `cache()` / `persist()`     | Mark a DataFrame to be cached for reuse     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0849872f-475d-428a-8df4-821680b26e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#⚡ Actions (Trigger Execution)\n",
    "These trigger execution of the transformations and return results or write output.\n",
    "| Action               | Description                                               |\n",
    "| -------------------- | --------------------------------------------------------- |\n",
    "| `show()`             | Displays first 20 rows (default)                          |\n",
    "| `collect()`          | Returns all rows as a list (use cautiously)               |\n",
    "| `count()`            | Returns number of rows                                    |\n",
    "| `take(n)`            | Returns first `n` rows                                    |\n",
    "| `first()`            | Returns the first row                                     |\n",
    "| `head()`             | Alias for `take(1)`                                       |\n",
    "| `foreach()`          | Applies a function to each row (no return)                |\n",
    "| `foreachPartition()` | Applies function to each partition                        |\n",
    "| `write()`            | Saves the DataFrame to storage (CSV, JSON, Parquet, etc.) |\n",
    "| `saveAsTable()`      | Saves DataFrame as a Hive or Delta table                  |\n",
    "| `toPandas()`         | Converts to Pandas DataFrame (only for small datasets)    |\n",
    "| `reduce()`           | Aggregates all elements using a function (RDD)            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e63164-8731-4caa-a27c-5028deb77162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ Advantages of Delta Lake over Traditional File Formats\n",
    "| Feature                        | Delta Lake 🟢                                         | Traditional Formats (CSV, JSON, Parquet) 🔴 |\n",
    "| ------------------------------ | ----------------------------------------------------- | ------------------------------------------- |\n",
    "| **ACID Transactions**          | ✅ Yes (ensures data reliability and consistency)      | ❌ No transactional guarantees               |\n",
    "| **Schema Evolution**           | ✅ Supports automatic schema updates (`mergeSchema`)   | ❌ Schema must be manually managed           |\n",
    "| **Time Travel**                | ✅ Access data as of a version or timestamp            | ❌ Not supported                             |\n",
    "| **Data Versioning**            | ✅ Built-in (every change tracked)                     | ❌ Not available                             |\n",
    "| **Upserts & Deletes**          | ✅ Easy with `MERGE`, `UPDATE`, `DELETE`               | ❌ Not natively supported                    |\n",
    "| **Unified Batch & Streaming**  | ✅ Same table supports both (via Structured Streaming) | ❌ Streaming needs separate handling         |\n",
    "| **Data Quality (Constraints)** | ✅ Supports `NOT NULL`, `CHECK`, custom constraints    | ❌ No constraint enforcement                 |\n",
    "| **Performance (Z-Ordering)**   | ✅ Data skipping & faster reads                        | ❌ Slower queries, no intelligent indexing   |\n",
    "| **Scalability & Reliability**  | ✅ Excellent for big data pipelines                    | ❌ Harder to manage at scale                 |\n",
    "| **Vacuum & Compaction**        | ✅ Built-in file cleanup and optimization              | ❌ Manual cleanup needed                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c54f4e-fca8-42e4-bfb7-f5d57fc78780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Error Message                                 | Likely Cause                              |\n",
    "| --------------------------------------------- | ----------------------------------------- |\n",
    "| `java.lang.OutOfMemoryError: Java heap space` | Insufficient memory for executor task     |\n",
    "| `GC overhead limit exceeded`                  | Too much time spent on garbage collection |\n",
    "| `ExecutorLostFailure`                         | Executor crashed due to memory pressure   |\n",
    "| `Python worker failed to connect back`        | Worker crash due to OOM                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee55227-2b20-4fc8-8068-38dc9ec23b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 What is AQE in PySpark?\n",
    "AQE (Adaptive Query Execution) is a feature in Spark (from version 3.0 onwards) that dynamically optimizes query plans at runtime, based on the actual data statistics collected during query execution — rather than relying solely on static compile-time plans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b98cc642-dec9-4f66-af07-261106265275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| AQE Optimization                               | Description                                                                 |\n",
    "| ---------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| ✅ **Dynamic Partition Pruning**                | Skips unnecessary partitions at runtime during joins                        |\n",
    "| ✅ **Dynamic Join Strategy Switching**          | Chooses **Broadcast Join** or **Sort-Merge Join** based on actual data size |\n",
    "| ✅ **Dynamic Coalescing of Shuffle Partitions** | Adjusts the number of shuffle partitions to prevent small tasks or skew     |\n",
    "| ✅ **Handling Data Skew**                       | Detects skewed partitions and splits them                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12faaffa-871a-4eb6-9c06-096e9ca3f01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DAtA SKEW\n",
    "Handling skewed data is a critical part of optimizing PySpark performance, especially for joins, groupBy, and aggregations. **Skewed data means that some keys have a disproportionately large number of records, causing task imbalance, longer job runtimes, and possible out-of-memory errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72915fa7-b490-49ed-a1a6-7886192c0462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"SkewedJoinHandling\").getOrCreate()\n",
    "\n",
    "# Skewed large orders dataset\n",
    "orders_data = [\n",
    "    (\"India\", \"A1\"),\n",
    "    (\"India\", \"A2\"),\n",
    "    (\"India\", \"A3\"),\n",
    "    (\"India\", \"A4\"),\n",
    "    (\"India\", \"A5\"),\n",
    "    (\"US\", \"B1\"),\n",
    "    (\"UK\", \"C1\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"order\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "df_orders.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd188976-0dee-4078-9f79-6b775f9427ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country_info_data = [\n",
    "    (\"India\", \"Asia\"),\n",
    "    (\"US\", \"America\"),\n",
    "    (\"UK\", \"Europe\")\n",
    "]\n",
    "\n",
    "country_info_schema = StructType([\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_country_info = spark.createDataFrame(country_info_data, schema=country_info_schema)\n",
    "df_country_info.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "846e27cf-9905-46fa-987d-c07d084b073e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#❌ 3. Problem with Normal Join (skew issue)\n",
    "\n",
    "\n",
    "If \"India\" had millions of rows, this join would be heavily skewed and inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6241f220-9911-4c1a-b3ef-b3080c6da54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skewed_join = df_orders.join(df_country_info, on=\"country\")\n",
    "df_skewed_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26b9fae3-fb75-468f-abe4-55849eefca9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ 4. Solution – Apply Salting Technique\n",
    "🔄 Step 1: Add Salt to df_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcaba64f-e4e8-4fe1-ab0d-414ad7aed36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders_salted = df_orders.withColumn(\n",
    "    \"country_salt\",\n",
    "    concat(col(\"country\"), lit(\"_\"), (rand() * 5).cast(\"int\"))\n",
    ")\n",
    "df_orders_salted.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a7cac9-ca36-4f11-b8da-2fe62e09227f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔄 Step 2: Duplicate df_country_info with Matching Salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98211a98-8fbf-4b58-bb0d-a97676c06c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create 5 copies with salt values from 0 to 4\n",
    "from functools import reduce\n",
    "\n",
    "salted_info_dfs = []\n",
    "for i in range(5):\n",
    "    salted_df = df_country_info.withColumn(\n",
    "        \"country_salt\",\n",
    "        concat(col(\"country\"), lit(f\"_{i}\"))\n",
    "    )\n",
    "    salted_info_dfs.append(salted_df)\n",
    "\n",
    "# Union all salted versions together\n",
    "df_country_info_salted = reduce(lambda df1, df2: df1.union(df2), salted_info_dfs)\n",
    "df_country_info_salted.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de71a3d4-6fd5-4aef-8f86-2b14508eb4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔄 Step 3: Join on country_salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0743db91-6ced-45b3-b907-83e195e1d638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_orders_salted.join(df_country_info_salted, on=\"country_salt\")\n",
    "df_final.display()\n",
    "df_final.select(\"country_salt\", \"order\", \"region\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4527d4d1-40cb-4621-860b-dfa1dfa412ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 What is a Broadcast Join in PySpark?\n",
    "A broadcast join is an optimized join strategy in Spark where the smaller DataFrame is sent (broadcasted) to all executors.\n",
    "This avoids shuffling the large dataset, resulting in much faster joins, especially in skewed or unbalanced joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9a4584-78da-460b-8caa-1e0b1adbbd80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🧠 When to Use Broadcast Join?\n",
    "✅ When one of the DataFrames is small (typically < 10 MB to 100 MB)\n",
    "✅ When joining a large fact table with a small dimension/lookup table\n",
    "✅ When you want to prevent data shuffle during join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b0ff39-b82d-4c1a-8135-8fb8f1dd2307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#✅ Benefits of Broadcast Join:\n",
    "| Advantage               | Description                                  |\n",
    "| ----------------------- | -------------------------------------------- |\n",
    "| ⚡ Fast performance      | No shuffle of large dataset                  |\n",
    "| 💾 Memory efficient     | Broadcasted dataset stored once per executor |\n",
    "| 🛠 Works well with skew | Avoids skew issues in many-to-one joins      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1668d33-d46d-4977-92f5-9597199bafa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# large_df.join(broadcast(small_df), on=\"key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b76c887-2406-4a8f-95fe-f4a7c37da1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_data = [\n",
    "    (\"India\", \"A1\"),\n",
    "    (\"India\", \"A2\"),\n",
    "    (\"US\", \"B1\"),\n",
    "    (\"UK\", \"C1\")\n",
    "]\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, [\"country\", \"order\"])\n",
    "df_orders.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55cbe3f5-1ce7-467e-b6cb-43b90e49cf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country_info_data = [\n",
    "    (\"India\", \"Asia\"),\n",
    "    (\"US\", \"America\"),\n",
    "    (\"UK\", \"Europe\")\n",
    "]\n",
    "\n",
    "df_country_info = spark.createDataFrame(country_info_data, [\"country\", \"region\"])\n",
    "df_country_info.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0615fac1-d662-497b-8be7-7d53124c9cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders.join(broadcast(df_country_info), on='country').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e09d1bc-6e7c-444e-b953-000f0b0690cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#🔍 What is Spill in Spark?\n",
    "Spill in Spark refers to the process where intermediate data is temporarily written to disk when it cannot fit into memory during operations like shuffle, aggregation, sort, or join.\n",
    "\n",
    "This is a safety mechanism to prevent OutOfMemory errors, but it can slow down performance since disk I/O is much slower than memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13d901a-57f4-4d6a-8f65-8a74362f9062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#📦 When Does Spill Happen?\n",
    "Spill typically occurs in the following scenarios:\n",
    "| Operation                         | Reason for Spill                                |\n",
    "| --------------------------------- | ----------------------------------------------- |\n",
    "| `groupBy`, `reduceByKey`, `agg()` | Intermediate data exceeds executor memory       |\n",
    "| `sort()`, `orderBy()`             | Sorting large datasets can exceed memory limits |\n",
    "| `join()`                          | Large shuffles or joins exceed memory           |\n",
    "| `shuffle`                         | Shuffle data too large to keep in-memory        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d93fd6-945a-4859-b5e2-346f1b936484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#⏳ What is Delta Lake’s Time Travel Feature?\n",
    "Time Travel in Delta Lake allows you to query, restore, or compare previous versions of a Delta table — like going back in time!\n",
    "This is possible because Delta Lake maintains a transaction log (_delta_log) that tracks all changes to the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53bd5fce-e475-4888-82ff-cdbc6bdf7e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#43. You are processing sales data. Group by product categories and create a list of all product names in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4081a1a-3102-40e0-a43b-82a11adbd5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Electronics\", \"Laptop\"), (\"Electronics\", \"Smartphone\"), (\"Furniture\", \"Chair\"), (\"Furniture\", \"Table\")]\n",
    "columns = [\"category\", \"product\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21bd9ad-c24e-47f7-b2b5-6681a195bc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adb0542-70ee-46d5-a70a-a4302fc5735e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('category').agg(collect_list('product')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e55763-29f0-46c1-9860-bc3cce688f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#44. You are analyzing orders. Group by customer IDs and list all unique product IDs each customer purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b3d505-b0b0-44cd-b362-16d711e9959d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(101, \"P001\"), (101, \"P002\"), (102, \"P001\"), (101, \"P001\")]\n",
    "columns = [\"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8725aee-d3fc-4cbb-8c9a-05e863057f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy('Customer_id').agg(collect_set('product_id').alias('Product_ID')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e3c4f4-4da7-4e34-9457-8c09b3f4ca35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#45. For customer records, combine first and last names only if the email address exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68047390-987f-447b-aa02-f7cd05541a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"John\", \"Doe\", \"john.doe@example.com\"), (\"Jane\", \"Smith\", None)]\n",
    "columns = [\"first_name\", \"last_name\", \"email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e3fbdd-eb33-4113-add2-26a0125892c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= df.filter(col(\"email\").isNotNull())\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359068fa-3346-4a35-86a0-b307d4cb457e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Full_Name', concat(col('first_name'), lit(' '), col('last_name'))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e46873-4c61-4921-914a-9210d094f251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Full_name', when(col('email').isNotNull(), concat(col('first_name'), lit(' '), col('last_name'))).otherwise(lit(' '))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1639737-e3d5-481d-9432-40f334ac8251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#46. You have a DataFrame containing customer IDs and a list of their purchased product IDs. Calculate the number of products each customer has purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08bd837-18f5-45c2-8969-5ab873fd7639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, [\"prod1\", \"prod2\", \"prod3\"]),\n",
    "    (2, [\"prod4\"]),\n",
    "    (3, [\"prod5\", \"prod6\"]),\n",
    "]\n",
    "myschema = \"customer_id INT ,product_ids array<STRING>\"\n",
    "\n",
    "df = spark.createDataFrame(data, myschema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b86f44b-4af3-447f-9d17-98a21e0a16a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Number_of_Products',size(col('product_ids'))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df509ab-b5b2-4e73-837c-5a452474635b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#47. You have employee IDs of varying lengths. Ensure all IDs are 6 characters long by padding with leading zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dfb726-7273-4eb2-8819-c0be69055f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"1\",),\n",
    "    (\"123\",),\n",
    "    (\"4567\",),\n",
    "]\n",
    "schema = [\"employee_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681e5d7a-5e9a-42a9-8fe6-caf52c7c0dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Employee_id',lpad(col('employee_id'),6,'0')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb181d4-a6f1-4c31-98df-4c7daa1948b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Employee_id',rpad(col('employee_id'),6,'0')).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eca1a95-b129-4607-a529-ed5122167405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#48. You need to validate phone numbers by checking if they start with \"91\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75630bda-8250-40c6-8714-387ebedc48f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"911234567890\",),\n",
    "    (\"811234567890\",),\n",
    "    (\"912345678901\",),\n",
    "]\n",
    "schema = [\"phone_number\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933aae09-98f8-415e-8e9d-0f66c58cb426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(substring(col('phone_number'),1,2)=='91').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d18c35-c09e-42c2-a859-6ad442e349db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#49. You have a dataset with courses taken by students. Calculate the average number of courses per student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746f83b7-c288-4f15-be19-94413faf0d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, [\"Math\", \"Science\"]),\n",
    "    (2, [\"History\"]),\n",
    "    (3, [\"Art\", \"PE\", \"Biology\"]),\n",
    "]\n",
    "schema = [\"student_id\", \"courses\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284b7ea4-f630-4a78-99ff-9394c93656e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Add a column that counts number of courses\n",
    "df = df.withColumn(\"Course_count\", size(col(\"courses\")))\n",
    "\n",
    "# Step 2: Compute average of the Course_count column\n",
    "df.agg(avg(\"Course_count\").alias(\"Course_avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aadc6ae0-fce7-4af7-865c-e79472b8f305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#50. You have a dataset with primary and secondary contact numbers. Use the primary number if available; otherwise, use the secondary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc274af4-72e8-4cd2-920d-fc5e304ada51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (None, \"1234567890\"),\n",
    "    (\"9876543210\", None),\n",
    "    (\"7894561230\", \"4567891230\"),\n",
    "]\n",
    "schema = [\"primary_contact\", \"secondary_contact\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38a7f9a-6ab4-43d6-9d5f-bfe5187746ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Phone_number', when(col('primary_contact').isNotNull(), col('primary_contact')).otherwise(col('secondary_contact'))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6a2727-dbf5-46e1-8118-720034f1b081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Contact',coalesce(col('primary_contact'),col('secondary_contact'))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1712c9-e992-4b94-b9eb-5b2d4b853240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"Coalesc_Col\", coalesce(col('primary_contact'), lit('Null'))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e14e2ff-97c1-44fd-87d3-79519dc29aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#DoneDoneDone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14572345-45a7-46ca-9c4f-6e4e3c9656c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#\n",
    "#51. You are categorizing product codes based on their lengths. If the length is 5, label it as \"Standard\"; otherwise, label it as \"Custom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9396a248-7b47-4129-aeff-cec435534865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"prod1\",),\n",
    "    (\"prd234\",),\n",
    "    (\"pr9876\",),\n",
    "]\n",
    "schema = [\"product_code\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42a3641-e05c-43a0-b0ce-dc9ba7cf3466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('Product_code',when(length(col('product_code'))==5,'Standard' ).otherwise('Custom')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d6dcf1-bf45-4385-914e-36f60d2667a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7643498775950620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Interview_Qestions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
